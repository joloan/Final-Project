---
title: "Problem Set 4"
author: "Karen Ngo"
subtitle: MGSC 310 Problem Set Template
output:
  html_document:
    df_print: paged
  html_notebook: default
---

```{r setup, include=FALSE}


library(knitr)

# As long as you are working in a Rstudio Project file, you shouldn't need to 'hard code' directories like this
# change to your own working directory
#knitr::opts_knit$set(root.dir = 'C:/Users/doosti/Desktop/MGSC_310')



options(width=70)
# if you want to prevent scientific format for numbers use this:
options(scipen=99)

# general rchunk code options
opts_chunk$set(tidy.opts=list(width.wrap=50),tidy=FALSE, size = "vsmall")
opts_chunk$set(message = FALSE,
               warning = FALSE,
               cache = TRUE,
               autodep = TRUE,
               cache.comments = FALSE,
               collapse = TRUE,
               fig.width = 5,
               fig.height = 4,
               fig.align='center')

```

```{r setup_2}

# load all your libraries here
library('tidyverse')
library('rsample')
library('tree')
library('caret')
library('ggplot2')
library('randomForest')
library('ggcorrplot') 
library('caTools')
library('car')
# note, do not run install.packages() inside a code chunk. install them in the console outside of a code chunk. 

```


## Question a

a) Use the IMDB 5000 dataset and the following code to clean and prepare the dataset for modeling.


```{r}

Cost <- read.csv("datasets/media prediction and its cost.csv")
Cost %>% glimpse()

CostC <- Cost[-sample(1:nrow(Cost), 55428), ]
CostClean<- CostC %>% mutate(food_category = fct_lump_min(food_category,125),
                             food_department= fct_lump_min(food_department,175),
                             promotion_name = fct_lump_min(promotion_name,125),
                             brand_name = fct_lump_min(brand_name,85),
                             store_city = fct_lump_min(store_city,100))

CostClean<- CostClean %>% mutate(food_category = as.factor(food_category),
                             food_department= as.factor(food_department),
                             food_family = as.factor(food_family),
                             promotion_name = as.factor(promotion_name),
                             sales_country = as.factor(sales_country),
                             marital_status = as.factor(marital_status),
                             education = as.factor(education),
                             member_card = as.factor(member_card),
                             occupation = as.factor(occupation),
                             houseowner = as.factor(houseowner),
                             brand_name = as.factor(brand_name),
                             recyclable_package = as.factor(recyclable_package),
                             low_fat = as.factor(low_fat),
                             store_type = as.factor(store_type),
                             store_city = as.factor(store_city),
                             store_state = as.factor(store_state),
                             coffee_bar = as.factor(coffee_bar),
                             video_store = as.factor(video_store),
                             salad_bar = as.factor(salad_bar),
                             prepared_food = as.factor(prepared_food),
                             florist = as.factor(florist),
                             media_type = as.factor(media_type))

#Summary Stats
CostClean %>%  glimpse()
summary(CostClean)
view(CostClean)
                               




```


```{r}

set.seed(765)
cost_split <- initial_split(CostClean, prop=0.8)
cost_train <- training(cost_split)
cost_test <- testing(cost_split)

```

LINEAR REGRESSION (NORMAL)
```{r}
model = lm(cost ~., 
           data = cost_train)

summary(model)

predict_train <- predict(model, newdata=cost_train)
predict_test <- predict(model, newdata=cost_test)

rmse_train <- RMSE(predict_train, cost_train$cost)
rmse_test <- RMSE(predict_test, obs = cost_test$cost)

rmse_train
rmse_test
```

LINEAR REGRESSION (NEW CLEAN DATA)
```{r}
vif(model) #Multicollinearity problem
test <- lm(cost~ video_store + prepared_food + coffee_bar + salad_bar + florist, data = cost_train)
vif(test) #Most likely these variables, need to drop due to multicollinearity

#Dropping suspected multicollinearity variables
newCost <- CostClean[,-30:-38]
#view(CostClean2)

cost_split_new <- initial_split(newCost, prop=0.8)
cost_train_new <- training(cost_split_new)
cost_test_new <- testing(cost_split_new)
newModel <- lm(cost~., data = cost_train_new)
summary(newModel) # R studios ignores variables with NA/ so metrics will be the same but with less variables

predict_train_new <- predict(newModel, newdata=cost_train_new)
predict_test_new <- predict(newModel, newdata=cost_test_new)

rmse_train_new <- RMSE(predict_train_new, cost_train_new$cost)
rmse_test_new <- RMSE(predict_test_new, obs = cost_test_new$cost)

rmse_train_new
rmse_test_new
```

DECISION TREE (CLEANER DATA SET)

```{r}
tree_cost <- tree(cost ~., data = cost_train_new)
plot(tree_cost)
text(tree_cost, digit=2, pretty = 0, cex=0.5)
```

```{r}
predict_cost_train <- predict(tree_cost, newdata=cost_train_new)
predict_cost_test <- predict(tree_cost, newdata=cost_test_new)

rmse_tree_train <- RMSE(predict_cost_train, cost_train_new$cost)
rmse_tree_test <- RMSE(predict_cost_test, cost_test_new$cost)

rmse_tree_train
rmse_tree_test
```


```{r}
cv.tree_cost <- cv.tree(tree_cost)
cv.tree_cost
plot(cv.tree_cost$size, cv.tree_cost$dev, type = 'b')
```

```{r}
size <- cv.tree_cost$size[which.min(cv.tree_cost$dev)]

pruned_tree_cost <- prune.tree(tree_cost, best = size)

prune_predict_cost_train <- predict(pruned_tree_cost, newdata=cost_train_new)
prune_predict_cost_test <- predict(pruned_tree_cost, newdata=cost_test_new)

prune_rmse_tree_train <- RMSE(prune_predict_cost_train, cost_train_new$cost)
prune_rmse_tree_test <- RMSE(prune_predict_cost_test, cost_test_new$cost)

prune_rmse_tree_train
prune_rmse_tree_test

```

RANDOM FOREST (CLEANER DATA SET)
```{r}
mtry <- tuneRF(newCost[-1],newCost$cost, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)

costRF<- randomForest(cost~.,
                      data = cost_train_new,
                      ntree = 175,
                      mtry = 15,
                      importance = TRUE)


print(costRF)
plot(costRF)


Cost_RF_train<-  predict(costRF, newdata = cost_train_new)
Cost_RF_test <- predict(costRF, newdata = cost_test_new)

RMSE(Cost_RF_train, cost_train_new$cost)
RMSE(Cost_RF_test, cost_test_new$cost)
```


SCALING DATA BY LOG
```{r}
logCost <- newCost %>% mutate(store_sales.in.millions. = log(store_sales.in.millions.),
                              store_cost.in.millions. = log(store_cost.in.millions.),
                              unit_sales.in.millions. = log(unit_sales.in.millions.),
                              total_children = log(total_children),
                              avg_cars_at.home.approx. = log(avg_cars_at.home.approx.),
                              num_children_at_home = log(num_children_at_home),
                              avg_cars_at.home.approx..1 = log(avg_cars_at.home.approx..1),
                              SRP = log(SRP),
                              gross_weight = log(gross_weight),
                              net_weight = log(net_weight),
                              units_per_case = log(units_per_case),
                              cost = log(cost))

#There are infinite numbers in the data set, this code will turn those inf values to NA values, which will then be dropped (ttps://www.r-bloggers.com/2022/10/how-to-replace-inf-values-with-na-in-r/)
logCost[sapply(logCost,is.infinite)] <- NA
logCost <- drop_na(logCost)

logCost <- logCost[,-9]
```


```{r}
set.seed(765)
cost_split_log <- initial_split(logCost, prop=0.8)
cost_train_log <- training(cost_split_log)
cost_test_log <- testing(cost_split_log)
```

LINEAR REGRESSION (LOG DATA)
```{r}
model_log = lm(cost ~., 
           data = cost_train_log)

summary(model_log)

predict_train_log <- predict(model_log, newdata=cost_train_log)
predict_test_log <- predict(model_log, newdata=cost_test_log)

rmse_train_log <- RMSE(predict_train_log, cost_train_log$cost)
rmse_test_log <- RMSE(predict_test_log, obs = cost_test_log$cost)

rmse_train_log
rmse_test_log
```

DECISION TREE (LOG DATA)
```{r}
tree_cost_log <- tree(cost ~., data = cost_train_log)
plot(tree_cost_log)
text(tree_cost_log, digit=2, pretty = 0, cex=0.5)
```


```{r}
predict_cost_train_log <- predict(tree_cost_log, newdata=cost_train_log)
predict_cost_test_log <- predict(tree_cost_log, newdata=cost_test_log)

rmse_tree_train_log <- RMSE(predict_cost_train_log, cost_train_log$cost)
rmse_tree_test_log <- RMSE(predict_cost_test_log, cost_test_log$cost)

rmse_tree_train_log
rmse_tree_test_log
```


```{r}
cv.tree_cost_log <- cv.tree(tree_cost_log)
cv.tree_cost_log
plot(cv.tree_cost_log$size, cv.tree_cost_log$dev, type = 'b')
```


```{r}
size_log <- cv.tree_cost_log$size[which.min(cv.tree_cost_log$dev)]

pruned_tree_cost_log <- prune.tree(tree_cost_log, best = size_log)

prune_predict_cost_train_log <- predict(pruned_tree_cost_log, newdata=cost_train_log)
prune_predict_cost_test_log <- predict(pruned_tree_cost_log, newdata=cost_test_log)

prune_rmse_tree_train_log <- RMSE(prune_predict_cost_train_log, cost_train_log$cost)
prune_rmse_tree_test_log <- RMSE(prune_predict_cost_test_log, cost_test_log$cost)

prune_rmse_tree_train_log
prune_rmse_tree_test_log
```

RANDOM FOREST (LOG DATA)
```{r}
mtry_log <- tuneRF(logCost[-1],logCost$cost, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m_log <- mtry_log[mtry_log[, 2] == min(mtry_log[, 2]), 1]
print(mtry_log)
print(best.m_log)

costRF_log<- randomForest(cost~.,
                      data = cost_train_log,
                      ntree = 175,
                      mtry = 29,
                      importance = TRUE)


print(costRF_log)
```

```{r}
Cost_RF_train_log <-  predict(costRF_log, newdata = cost_train_log)
Cost_RF_test_log <- predict(costRF_log, newdata = cost_test_log)

RMSE(Cost_RF_train_log, cost_train_log$cost)
RMSE(Cost_RF_test_log, cost_test_log$cost)
```
